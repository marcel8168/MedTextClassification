{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training set, validation set and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.json', 'valid.json']\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "\n",
    "kaggle.api.authenticate()\n",
    "data_path = \"./datasets/\"\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    print(f\"Directory created: {data_path}\")\n",
    "    \n",
    "kaggle.api.dataset_download_file('marcelhiltner/pubmed-human-veterinary-medicine-classification', file_name=\"train.json\", path=data_path)\n",
    "kaggle.api.dataset_download_file('marcelhiltner/pubmed-human-veterinary-medicine-classification', file_name=\"valid.json\", path=data_path)\n",
    "zip_paths = [f\"{data_path}valid.json.zip\", f\"{data_path}train.json.zip\"]\n",
    "for path in zip_paths:\n",
    "    with zipfile.ZipFile(path, \"r\") as z:\n",
    "        z.extractall(data_path)\n",
    "    os.remove(path)\n",
    "print(os.listdir(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "try:\n",
    "    train_set = pd.read_json(f\"{data_path}train.json\", orient=\"records\")\n",
    "    print(\"Data loaded successfully: train.json\")\n",
    "    print(f\"Shape: {train_set.shape}\")\n",
    "    val_set = pd.read_json(f\"{data_path}valid.json\", orient=\"records\")\n",
    "    print(\"Data loaded successfully: valid.json\")\n",
    "    print(f\"Shape: {val_set.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "from Source_code.z_utils.BERTClassifier import BERTClassifier\n",
    "from Source_code.z_utils.RoBERTaClassifier import RoBERTaClassifier\n",
    "from Source_code.z_utils.DeBERTaClassifier import DeBERTaClassifier\n",
    "from Source_code.z_utils.BlueBERTClassifier import BlueBERTClassifier\n",
    "from Source_code.z_utils.XLNetClassifier import XLNetClassifier\n",
    "from Source_code.z_utils.data_preparing import get_dataloader\n",
    "from Source_code.z_utils.Dataset import Dataset\n",
    "from Source_code.z_utils.loss_fn import loss_fn\n",
    "from Source_code.z_utils.train import train_model\n",
    "from Source_code.z_utils.plot import plot\n",
    "from Source_code.z_utils.global_constants import *\n",
    "\n",
    "\n",
    "models = [BERTClassifier(\"bert-base-uncased\"),\n",
    "          RoBERTaClassifier(\"roberta-base\"),\n",
    "          DeBERTaClassifier(\"microsoft/deberta-base\"),\n",
    "          BlueBERTClassifier(\"bionlp/bluebert_pubmed_uncased_L-24_H-1024_A-16\"),\n",
    "          XLNetClassifier(\"xlnet-large-cased\")\n",
    "         ]\n",
    "\n",
    "model_histories = []\n",
    "\n",
    "for model in models:\n",
    "    print(\"=\" * 30)\n",
    "    print(f'Model {model.checkpoint} started.')\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_dataloader = get_dataloader(train_set.title_abstract, train_set.labels, model.tokenizer, TRAIN_BATCH_SIZE, MAX_LEN)\n",
    "    val_dataloader = get_dataloader(val_set.title_abstract, val_set.labels, model.tokenizer, VAL_BATCH_SIZE, MAX_LEN)\n",
    "       \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # train and validation accuracies plot\n",
    "    metrics, losses, sliding_accuracies, info_best_model, run_times = train_model(model, train_dataloader, val_dataloader, TRAIN_BATCH_SIZE, loss_fn, optimizer, device, scheduler, EPOCHS)\n",
    "    \n",
    "    if not os.path.exists(PATH_SAVED_METRICS):\n",
    "        os.makedirs(PATH_SAVED_METRICS)\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_json(f\"{PATH_SAVED_METRICS}metrics_{model.checkpoint[model.checkpoint.find('/')+1:]}.json\")\n",
    "    train_losses_df = pd.DataFrame([{\"loss\": elem[\"loss\"].detach().item(), \"epoch\": elem[\"epoch\"]}  for elem in losses])\n",
    "    train_losses_df.to_json(f\"{PATH_SAVED_METRICS}training_losses_{model.checkpoint[model.checkpoint.find('/')+1:]}.json\")\n",
    "    train_accs_df = pd.DataFrame(sliding_accuracies)\n",
    "    train_accs_df.to_json(f\"{PATH_SAVED_METRICS}training_accuracies_{model.checkpoint[model.checkpoint.find('/')+1:]}.json\")\n",
    "    with open(f\"{PATH_SAVED_METRICS}info_best_model_{model.checkpoint[model.checkpoint.find('/')+1:]}.txt\", \"w\") as text_file:\n",
    "        text_file.write(info_best_model)\n",
    "    run_times_df = pd.DataFrame(run_times)\n",
    "    run_times_df.to_json(f\"{PATH_SAVED_METRICS}training_times_{model.checkpoint[model.checkpoint.find('/')+1:]}.json\")\n",
    "        \n",
    "    model_histories.append({\"model\": model.checkpoint, \"metrics\": metrics_df, \"train_losses\": train_losses_df, \"train_accs\": train_accs_df, \"info_best_model\": info_best_model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy and loss history of training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "for model_hist in model_histories:\n",
    "    print(\"=\" * 30)\n",
    "    print(f'Model {model_hist[\"model\"]}')\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    metrics_df = model_hist[\"metrics\"]\n",
    "    train_losses_df = model_hist[\"train_losses\"]\n",
    "    train_accs_df = model_hist[\"train_accs\"]\n",
    "    info_best_model = model_hist[\"info_best_model\"]\n",
    "    \n",
    "    # training and validation accuracy history\n",
    "    data_list = [metrics_df[\"train_acc\"], metrics_df[\"val_acc\"]]\n",
    "    label_list = [\"Training accuracy\", \"Validation accuracy\"]\n",
    "    all_accs = pd.concat(data_list)\n",
    "    plot(data_list, label_list, \n",
    "         \"Training and validation accuracy history\", \n",
    "         \"Accuracy\", \n",
    "         \"Evaluation number\", \n",
    "         vlines=[i * len(metrics_df[\"train_acc\"]) / EPOCHS for i in range(1, EPOCHS)],\n",
    "         ylim=[all_accs.min(), all_accs.max()],\n",
    "         model_name=model.checkpoint\n",
    "        )\n",
    "        \n",
    "    # training and validation loss history\n",
    "    data_list = [metrics_df[\"train_loss\"], metrics_df[\"val_loss\"]]\n",
    "    label_list = [\"Training loss\", \"Validation loss\"]\n",
    "    all_accs = pd.concat(data_list)\n",
    "    plot(data_list, label_list, \n",
    "         \"Training and validation loss history\", \n",
    "         \"Loss\", \n",
    "         \"Evaluation number\", \n",
    "         vlines=[i * len(metrics_df[\"train_loss\"]) / EPOCHS for i in range(1, EPOCHS)],\n",
    "         ylim=[all_accs.min(), all_accs.max()],\n",
    "         model_name=model.checkpoint\n",
    "        )\n",
    "    \n",
    "    # training loss history\n",
    "    plot([train_losses_df[\"loss\"]], \n",
    "         [\"Training loss\"], \n",
    "         \"Training loss of all epochs\", \n",
    "         \"Loss\", \n",
    "         \"Number of training samples\", \n",
    "         vlines=[i * len(train_losses_df[\"loss\"]) / EPOCHS for i in range(1, EPOCHS)],\n",
    "         ylim=[train_losses_df[\"loss\"].min(), train_losses_df[\"loss\"].max()],\n",
    "         model_name=model.checkpoint\n",
    "        )\n",
    "        \n",
    "    # training loss history\n",
    "    plot([train_accs_df[\"accuracy\"]],\n",
    "         [\"Training accuracy\"],\n",
    "         \"Training accuracy per batch of all epochs\",\n",
    "         \"Accuracy\", \n",
    "         \"Number of training samples\",\n",
    "         vlines=[i * len(train_losses_df[\"loss\"]) / EPOCHS for i in range(1, EPOCHS)],\n",
    "         ylim=[train_accs_df[\"accuracy\"].min(), train_accs_df[\"accuracy\"].max()],\n",
    "         model_name=model.checkpoint\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
