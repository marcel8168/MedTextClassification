{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the PMC-Patients dataset\n",
    "Link to paper: https://arxiv.org/abs/2202.13876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"zhengyun21/PMC-Patients\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "patients_summaries = dataset.to_pandas()\n",
    "patients_summaries.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_summaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the average length of all titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_lengths = [len(title) for title in patients_summaries.title]\n",
    "pd.Series(title_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.hist(title_lengths, bins=100, label=\"count\")\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Title Length Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the average length of all patient summaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_lengths = pd.Series([len(summary) for summary in patients_summaries.patient])\n",
    "summary_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "quantile_1 = summary_lengths.quantile(0.01)\n",
    "quantile_99 = summary_lengths.quantile(0.99)\n",
    "index = summary_lengths[(summary_lengths >= quantile_99)|(summary_lengths <= quantile_1)].index\n",
    "\n",
    "summary_lengths_cleaned = summary_lengths.drop(index, inplace=False)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.hist(summary_lengths_cleaned, bins=200, label=\"count\")\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Patient Summary Length Distribution (trimmed)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random subsets for analysing the veterinary content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_summaries_samples = patients_summaries.sample(n=100, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_summaries_samples.to_json(\"patients_summaries_samples.json\", orient=\"records\", lines=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual analysis of these random generated sample sets led to the following results:\n",
    "\n",
    "|Sample Set|Veterinary Proportion|Extrapolation|\n",
    "|:-----|:--------|:--------|\n",
    "|Patients Summaries|1%|~ 1,670|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis by fine-tuned BlueBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import os\n",
    "\n",
    "from Source_code.z_utils.BlueBERTClassifier import BlueBERTClassifier\n",
    "\n",
    "\n",
    "kaggle.api.authenticate()\n",
    "data_path = \"./models/\"\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    print(f\"Directory created: {data_path}\")\n",
    "models = {}\n",
    "\n",
    "file_name = \"bluebert_pubmed_uncased_L-24_H-1024_A-16.pt\"\n",
    "target_path = f\"{data_path}{file_name}\"\n",
    "    \n",
    "if not os.path.exists(target_path):\n",
    "    slug = \"bluebert-large-pubmed\"\n",
    "    kaggle.api.model_instance_version_download_cli(f\"marcelhiltner/{slug}/pytorch/{slug}/1\", data_path, untar=True)\n",
    "\n",
    "blueBERT = torch.load(target_path)\n",
    "blueBERT.eval()\n",
    "print(\"BlueBERT loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vet_predictions_as_json(predictions, texts, filename):\n",
    "    label_preds = [torch.argmax(tensor) for tensor in predictions]\n",
    "    label_preds = torch.stack(label_preds)\n",
    "    vet_preds = (label_preds == 1).nonzero(as_tuple=True)[0].numpy(force=True)\n",
    "    vet_preds\n",
    "    vet_texts = texts.iloc[texts.index.isin(vet_preds)]\n",
    "    probs = pd.Series(torch.stack([torch.max(tensor) for tensor in predictions]).numpy(force=True), name=\"probability\")\n",
    "    probs = probs.iloc[probs.index.isin(vet_preds)]\n",
    "    vet_df = pd.concat([vet_texts, probs], axis=1)\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(vet_df.to_json(orient=\"records\")[1:-1].replace('},{', '} {'))\n",
    "        \n",
    "    print(f\"Predictions have been saved to {filename}.\")\n",
    "    \n",
    "    return vet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Source_code.z_utils.data_preprocessing import preprocess_text\n",
    "from Source_code.z_utils.predict import predict\n",
    "\n",
    "\n",
    "summary_texts = patients_summaries.patient.sample(n=10000, random_state=42)\n",
    "summary_texts.reset_index(drop=True, inplace=True)\n",
    "summary_texts_pp = [preprocess_text(text) for text in summary_texts]\n",
    "\n",
    "blueBERT.to(device)\n",
    "blueBERT.eval()\n",
    "summary_predictions, _ = predict(blueBERT, texts=summary_texts_pp, device=device)\n",
    "\n",
    "summary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_predictions_t = torch.tensor(summary_predictions)\n",
    "label_preds = [torch.argmax(tensor) for tensor in summary_predictions_t]\n",
    "label_preds = torch.stack(label_preds)\n",
    "vet_preds = (label_preds == 1).nonzero(as_tuple=True)[0].numpy(force=True)\n",
    "vet_preds\n",
    "vet_texts = summary_texts.iloc[summary_texts.index.isin(vet_preds)]\n",
    "probs = pd.Series(torch.stack([torch.max(tensor) for tensor in summary_predictions_t]).numpy(force=True), name=\"probability\")\n",
    "probs = probs.iloc[probs.index.isin(vet_preds)]\n",
    "vet_df = pd.concat([vet_texts, probs], axis=1)\n",
    "\n",
    "filename = \"summaries_vet.json\"\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(vet_df.to_json(orient=\"records\")[1:-1].replace('},{', '} {'))\n",
    "    \n",
    "print(f\"Predictions have been saved to {filename}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
